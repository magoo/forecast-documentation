# Forecast Training ðŸ”­ðŸ‹ï¸
This will be required reading or the basis of training for certain forecasts.

Research suggests that prediction improves with probabilitic training, practice, and certainty calibration. It has also shown that very minor amounts of training have a big effect on a forecaster. Training is one of many methods to improve prediction skills.

The following draws heavily from  the "Summary and discussion of results" of [Developing expert political judgment](http://journal.sjdm.org/16/16511/jdm16511.html) and my [forecast reading material](READING.md).

## Outside View vs Inside View
Individuals with a close up view of a problem often degrade their own ability to forecast it correctly. Yes, that's true, experts are often subject to _worse_ predictive capability resulting from their familiarity with a subject.

*Start with an outside view*. Consider the "outside view" of your scenario by comparing scenarios that are similar to the one you are interested in. For instance, the outcome of a supreme court case. Instead of starting with an understanding of the case itself, you should get an understanding of how often the Supreme Court reverses a decision or not, especially how often they reverse decisions brought by specific, lower courts.

This is often called "finding the base rate".

*Bring in the inside view*. Once you have your base rate well understood, only then begin to consider the specifics of your scenario. For instance, legal theory being used in the case. The previous opinions of the court. The history of the judges on similar subjects presented to them. How much does this influence the "base rate"?

## Avoid the Availability Heuristic.
The [availability heuristic](https://en.wikipedia.org/wiki/Availability_heuristic) influences and biases a forecast due to a shortcut most thought processes take: _if you remember it, it must be important_. Keep in mind that the events in your memory do not represent all available data, nor does it represent the most important data necessary for a good forecast. It just happens to be your memory at the time.

For instance, a violent plane crash may bias people against flying, when flights are historically known to be very safe.

## Identify your stubborness.
The bias of [belief perseverance](https://en.wikipedia.org/wiki/Belief_perseverance) describes a belief you may have in spite of information that directly contradicts it. The causes of this bias are unclear. Your ability to suspend a belief as you explore opposing arguments is an important skill. It's also important to avoid bold and confident claims when you lack supporting evidence, or have not thoroughly explored an opposing side.

In once extreme case, psychologists joined a cult, and observed that its members clung to their beliefs even _after_ the world did not end on a given date.

> Festinger, Leon; et al. (1956). When Prophecy Fails. Minneapolis: University of Minnesota Press.


## CHAMPS KNOW

|Letter   | Probabilistic Reasoning Principal   |
|:--:|:--:|
|  C |  Comparison classes|
|  H |  Hunt for the right information |
|  A | Adjust and update forecasts when appropriate  |
|  M |  Mathematical and statistical models |
|  P |  Post-mortem analysis |
|  S |  Select the right level of effort to devote to each question |
|    | Political Reasoning Principal  |
|  K |  Know the power players and their preferences|
|  N |  Norms and protocols of institutions |
|  O | Other perspectives should inform forecasts  |
|  W | Wildcards, accidents, and black swans |

### Comparison Classes ([link](https://en.wikipedia.org/wiki/Reference_class_forecasting))
Think through comparison classes to narrow in your forecast based on similar, comparable data.

1. Assume that your scenario is not entirely unique, and find comparables.
2. Identify what aspects of the scenario _do_ make it unique.
3. Explore the comparable scenarios and any data available, and determine how different they actually are by the estimating the value of those differences.

For instance: Estimating attendance at a performer's first headliner gig. Can you explore data from concerts at the same venue, type of music, or audience demographic? Maybe their previous, non-headline shows will give some basis for reasoning as well.

### Hunt for the right information
Immediately hunt for information that contradicts your gut feelings. Understand all nuance in the question statement. Be confident that you understand any nuance in the judgement criteria that may impact the outcome.

### Adjust and update forecasts when appropriate
If your scenario allows you to update a forecast over time, it will help. Not only is the progression of time a new source of information, but being able to "sleep on it" will allow you to apply critical thinking to a scenario with fresh perspective, or apply any other information you have gained.

### Mathematical and Statistical Models
When possible, bust out your spreadsheet and calculator and explore the historical data involved with your scenario. Understand the limitations of mechanical modeling, as well as your own subjective forecasting. You, as a forecaster, are subject to bias. However, a mechanical model might not see the data it needs to capture probability correctly.

### Post-Mortem Analysis
Value the "busted" forecasts! They are precious. What influenced a forecast that was ultimately incorrect? What reasoning was incorrect, or undervalued? What data would have valuable to have, in hindsight?

### Select the right level of effort to devote to each question
Very intentionally ask questions about the scenario. Decide which ones deserve your effort to research and answer them.

### Know the power players and their institutions
(Related to political forecasting)

### Norms and protocols of institutions
(Related to political forecasting)

### Other perspectives should inform forecasts
How would someone else look at this scenario? Would someone invested in this scenario view it differently? What about people invested in different outcomes, what questions or data would they rely on?

### Wildcards, accidents, and black swans
Always, always leave room for the complete shocker. Consider that we always view black swans as explainable events *after* they've punched us in the jaw. If you express certainty (0%/100%), you are likely opening yourself up for tragedy. The world is full of surprises, disappointment, accidents, and the unknown unknown.

---

## The Absurdity Test
This approach to forecasting uses absurd, strawman answers or questions a starting point that helps narrow down a forecast from a universe of possibilities. If you are stuck with massive uncertainty or an unwilling forecaster, this is a place to begin.

If forecasting an interval (a number between x and y), you can choose an extremely large or small number and immediately face disagreement.

For instance: "How many cookies are in this jar?". If presented with an answer of `-999999-999999`, you will disagree immediately if you can easily see at least _some_ cookies, and cannot fathom that there couldn't possible be more, than say, 20. You have then narrowed your estimate to `1-20` and can use further techniques to narrow it further.

In a probability distribution, you can split probability between outcomes. For instance, "Will I die today? Yes/No" can be evenly split as an absurdity test as 0%/100% or 100%/0%. This will only get weird if the person truly believes they are immortal or going to die that day. Otherwise, they may resort to reason as they compete with absurdity.

> This strategy is referenced in The British Journal of Psychology, Volume 12, and "How to Measure Anything".

## Forced Odds
In a probability distribution of outcomes, you can simply divide the outcomes evenly and see if you experience disagreement with the result. For instance: "Will it rain tomorrow? Yes/No". Split the outcomes evenly, such as 50%/50%, or 25%/25%/25%/25% with four outcomes, etc. You will likely experience disagreement if you have subjective data that argues with even odds. For instance, if you live in a desert, or the rainforest, you will likely disagree with even odds. Similarly, if you have access to any weather data or competing opinions.

---

# Online Training
Participation in something continuous like the [Good Judgement Open](https://www.gjopen.com/) will help keep forecasting skills sharp. Otherwise, varying levels of rigorous training below are available.

- https://good-judgment.thinkific.com/ (recommended)
- https://www.hubbardresearch.com/shop/calibration-webinar-quantify-uncertainty/ (enterprise)
- https://www.evidentmethod.com/training/ (free / enterprise)
- http://confidence.success-equation.com/
